{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZZ7oYtNPgjz"
   },
   "source": [
    "# Автоматическое распознавание новообразований в \"русском английском\" текстов разных жанров #\n",
    "## Анастасия Буракова, БКЛ 181 ##\n",
    "\n",
    "**Описание:** программа распознает в текстовых документах новообразования из [собранного корпуса](https://github.com/nstsi/find_neologisms/blob/main/data/neologisms_table.csv)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b6zN0QHKQLKX"
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EarOFVJ5RSCb"
   },
   "source": [
    "### 1. Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-GvnluDQcFg"
   },
   "source": [
    "Чтобы **проверить программу на тренировочном файле** DAr_8_1.txt, запустите\n",
    "код ниже и\n",
    "пропустите следующую ячейку:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LtGTlDLQQl-_"
   },
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "\n",
    "FILE_TO_PROCESS = 'DAr_8_1.txt'\n",
    "FILE_TO_PROCESS_PATH: str = f'{DATA_FOLDER}{FILE_TO_PROCESS}'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-yzw8i4Q6g7"
   },
   "source": [
    "Чтобы программа обработала **ваш текст** на английском языке, переведите его в\n",
    "формат .txt и положите в папку data, находящуюся в одной папке с этой программой. Далее запустите код ниже и введите имя вашего файла в формате filename.txt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fX4TGGBSWPiG"
   },
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "FILE_TO_PROCESS = input()\n",
    "FILE_TO_PROCESS_PATH: str = f'{DATA_FOLDER}{FILE_TO_PROCESS}'\n",
    "check_format = re.findall(r'.+\\.txt', FILE_TO_PROCESS)\n",
    "while not os.path.exists(FILE_TO_PROCESS_PATH) or not check_format:\n",
    "    print(f'Положите файл в формате .txt в папку data, находящуюся в одной'\n",
    "          f'папке с этой программой, и введите имя вашего файла'\n",
    "          f'в формате filename.txt.')\n",
    "    FILE_TO_PROCESS = input()\n",
    "FILE_TO_PROCESS_PATH: str = f'{DATA_FOLDER}{FILE_TO_PROCESS}'\n",
    "check_format = re.findall(r'.+\\.txt', FILE_TO_PROCESS)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOiG0--uYPR2"
   },
   "source": [
    "*Запуск **всех** следующих ячеек **обязателен** для исправной работы\n",
    "программы.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wl2eupRsQWKh"
   },
   "source": [
    "FILE_TO_PROCESS_NAME = FILE_TO_PROCESS[:-4]\n",
    "RESULTS_FILE: str = f'{FILE_TO_PROCESS_NAME}_results.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWUT-QXWRrgN"
   },
   "source": [
    "### 2. Подготовка данных\n",
    "Сначала прочитаем и очистим рассматриваемый текст, а также разделим его на предложения:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eVeGBcUORw7t"
   },
   "source": [
    "def read_text(filename: str) -> str:\n",
    "    with open(filename, encoding='utf-8') as txt:\n",
    "        text_ = txt.read()\n",
    "    return text_\n",
    "\n",
    "\n",
    "def make_one_line(text_: str) -> str:\n",
    "    text_ = re.sub('\\n', ' ', text_)\n",
    "    return text_\n",
    "\n",
    "\n",
    "def split_in_sentences(text_: str) -> list:\n",
    "    sentences_ = nltk.tokenize.sent_tokenize(text_)\n",
    "    return sentences_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbQrPNFARklE"
   },
   "source": [
    "Теперь создадим из корпуса новообразований датафрейм: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OwRpuvs6Rgjv"
   },
   "source": [
    "def make_neologisms_df():\n",
    "    neologisms_ = pd.read_csv('data/neologisms_table.csv', delimiter=',')\n",
    "    return neologisms_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIMpegx3SKOo"
   },
   "source": [
    "### 3. Обработка данных\n",
    "Теперь проверим текст на наличие каждого новообразования и добавим в новый датафрейм параметры каждого найденного новообразования, первое предложение, в котором новообразование встретилось в данном тексте, а также предшествующее и следующее предложений (контекст).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B-spXOa0SNJL"
   },
   "source": [
    "def find_neologisms(text_one_line_: str,\n",
    "                    sentences_: list,\n",
    "                    neologisms_):\n",
    "\n",
    "    # Названия колонок берем из существующего дф и добавляем еще 3 колонки:\n",
    "    column_names = neologisms_.columns.tolist()\n",
    "    columns_add = ['current_sentence', 'previous_sentence', 'next_sentence']\n",
    "    for elem in columns_add:\n",
    "        column_names.append(elem)\n",
    "\n",
    "    # Создаем датафрейм с этими колонками:\n",
    "    results = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    # Идем по строкам дф с новообразованиями и ищем их в тексте:\n",
    "    for index, row in neologisms_.iterrows():\n",
    "\n",
    "    # Если находим, сохраняем контекст:\n",
    "        if row['word'] in text_one_line_:\n",
    "            for i, sentence in enumerate(sentences_):\n",
    "\n",
    "    # Текущее предложение есть всегда:\n",
    "                if row['word'] in sentence:\n",
    "                    current_sentence = sentence\n",
    "\n",
    "    # А вот предыдушего или следующего может не быть:\n",
    "                    try:\n",
    "                        previous_sentence = sentences_[i-1]\n",
    "                    except IndexError:\n",
    "                        previous_sentence = 'пусто'\n",
    "\n",
    "                    try:\n",
    "                        next_sentence = sentences_[i+1]\n",
    "                    except IndexError:\n",
    "                        next_sentence = 'пусто'\n",
    "\n",
    "    # Записываем все параметры новообразования:\n",
    "                    res_list = [row['word'],\n",
    "                                row['meaning'],\n",
    "                                row['linguistic process'],\n",
    "                                row['mistake location'],\n",
    "                                row['mistake type'],\n",
    "                                row['L1 interference'],\n",
    "                                row['source'],\n",
    "                                row['enTenTen18'],\n",
    "                                current_sentence,\n",
    "                                previous_sentence,\n",
    "                                next_sentence]\n",
    "\n",
    "    # И добавляем их к датафрейму с результатами:\n",
    "                    df_length = len(results)\n",
    "                    results.loc[df_length] = res_list\n",
    "                    break  # Берём только первое вхождение\n",
    "\n",
    "    return results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEizJmH9SUQ-"
   },
   "source": [
    "### 4. Запись результатов\n",
    "Если ни одно новообразование не обнаружено в тексте, эта функция выведет в консоль сообщение типа: «В тексте DAr_8_1.txt не были найдены новообразования».\n",
    "\n",
    "Если образования были обнаружены, эта функция создаст файл результатов и запишет туда все полученные данные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9GTiLbkKSW9k"
   },
   "source": [
    "def save_to_file(results, results_filename, text_filename):\n",
    "\n",
    "    if results.empty:\n",
    "        print(f'В тексте {text_filename} не были найдены новообразования.')\n",
    "\n",
    "    else:\n",
    "        with open(results_filename, 'w', encoding='utf-8') as f:\n",
    "\n",
    "            f.write(f'В тексте {text_filename} были найдены следующие '\n",
    "                    f'новообразования:')\n",
    "            f.write('\\n')\n",
    "\n",
    "            for i in range(results.shape[0]):\n",
    "\n",
    "                f.write(f'{i+1}.')\n",
    "                f.write('\\n')\n",
    "\n",
    "                word = results.iloc[i]['word']\n",
    "                meaning = results.iloc[i]['meaning']\n",
    "                ling_proc = results.iloc[i]['linguistic process']\n",
    "                mistake_loc = results.iloc[i]['mistake location']\n",
    "                print(mistake_loc)\n",
    "                mistake_type = results.iloc[i]['mistake type']\n",
    "                print(mistake_type)\n",
    "                l1_interference = results.iloc[i]['L1 interference']\n",
    "                en18 = results.iloc[i]['enTenTen18']\n",
    "\n",
    "                f.write(f'• Новообразование: {word} | '\n",
    "                        f'Значение: {meaning} | '\n",
    "                        f'Лингвистический процесс: {ling_proc} | '\n",
    "                        f'Локация ошибки: {mistake_loc} | '\n",
    "                        f'Тип ошибки: {mistake_type} | '\n",
    "                        f'Возможность L1 interference: {l1_interference} | '\n",
    "                        f'Количество вхождений в enTenTen18: {en18}')\n",
    "                f.write('\\n')\n",
    "\n",
    "                curr_sent = results.iloc[i]['current_sentence']\n",
    "                f.write(f'• Предложение: {curr_sent}')\n",
    "                f.write('\\n')\n",
    "\n",
    "                prev_sent = results.iloc[i]['previous_sentence']\n",
    "                next_sent = results.iloc[i]['next_sentence']\n",
    "                context = f'{prev_sent} {curr_sent} {next_sent}'\n",
    "                clean_context = re.sub(' пусто', '', context)\n",
    "\n",
    "                f.write(f'• Контекст: {clean_context}')\n",
    "                f.write('\\n')\n",
    "\n",
    "                source = results.iloc[i]['source']\n",
    "                f.write(f'• Другие тексты с этим новообразованием: {source}')\n",
    "                f.write('\\n')\n",
    "\n",
    "                if results.iloc[i][0] == results.iloc[-1][0]:\n",
    "                    print(f'Найденные новообразования записаны в файл '\n",
    "                          f'{results_filename}.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKva2vehSj2N"
   },
   "source": [
    "### 5. Объединяем всё вместе:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qgxWP3DNSdvK"
   },
   "source": [
    "# Читаем текст:\n",
    "text = read_text(FILE_TO_PROCESS_PATH)\n",
    "\n",
    "# Записываем текст одной строкой:\n",
    "text_one_line = make_one_line(text)\n",
    "\n",
    "# Делим текст на предложения:\n",
    "sentences = split_in_sentences(text_one_line)\n",
    "\n",
    "# Создаем из корпуса новообразований датафрейм:\n",
    "neologisms = make_neologisms_df()\n",
    "\n",
    "# Ищем новообразования:\n",
    "results_df = find_neologisms(text_one_line, sentences, neologisms)\n",
    "\n",
    "# Записываем результаты в файл и выводим в консоль итог работы программы\n",
    "save_to_file(results_df, RESULTS_FILE, FILE_TO_PROCESS_NAME)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}